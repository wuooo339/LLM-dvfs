#!/usr/bin/env python3
"""
Disaggregated Prefill+Decode æ‰¹é‡æµ‹è¯•è„šæœ¬
æµ‹è¯•åˆ†ç¦»å¼æ¶æ„ï¼ˆPrefill å’Œ Decode åˆ†ç¦»ï¼‰çš„æ€§èƒ½
"""

import requests
import json
import time
import threading
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ gpu_monitor
sys.path.append(str(Path(__file__).parent.parent.parent))
try:
    from gpu_monitor import GPUMonitor
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥ GPUMonitorï¼ŒGPUç›‘æ§åŠŸèƒ½å°†ä¸å¯ç”¨")
    GPUMonitor = None

class DisaggBatchClient:
    """Disaggregated æ¶æ„æ‰¹é‡æµ‹è¯•å®¢æˆ·ç«¯"""
    def __init__(self, proxy_url="http://localhost:8000", prefill_url="http://localhost:8100", decode_url="http://localhost:8200"):
        self.proxy_url = proxy_url
        self.prefill_url = prefill_url
        self.decode_url = decode_url
        self.session = requests.Session()
        
    def estimate_tokens(self, text):
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2è®¡ç®—ï¼Œè‹±æ–‡æŒ‰1è®¡ç®—ï¼‰"""
        return sum(2 if '\u4e00' <= char <= '\u9fff' else 1 for char in text)
    
    def health_check(self):
        """æ£€æŸ¥æ‰€æœ‰æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        print("æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€...")
        all_healthy = True
        
        # æ£€æŸ¥ä»£ç†æœåŠ¡å™¨
        try:
            response = self.session.get(f"{self.proxy_url}/health", timeout=5)
            if response.status_code == 200:
                health_data = response.json()
                print(f"  âœ… ä»£ç†æœåŠ¡å™¨: æ­£å¸¸")
                print(f"     Prefillå®ä¾‹æ•°: {health_data.get('prefill_instances', 0)}")
                print(f"     Decodeå®ä¾‹æ•°: {health_data.get('decode_instances', 0)}")
                print(f"     Prefillåœ°å€: {health_data.get('prefill_addrs', [])}")
                print(f"     Decodeåœ°å€: {health_data.get('decode_addrs', [])}")
            else:
                print(f"  âŒ ä»£ç†æœåŠ¡å™¨: å¼‚å¸¸ (çŠ¶æ€ç  {response.status_code})")
                all_healthy = False
        except Exception as e:
            print(f"  âŒ ä»£ç†æœåŠ¡å™¨: æ— æ³•è¿æ¥ ({e})")
            all_healthy = False
        
        # æ£€æŸ¥ Prefill æœåŠ¡å™¨
        try:
            response = self.session.get(f"{self.prefill_url}/health", timeout=5)
            if response.status_code == 200:
                print(f"  âœ… PrefillæœåŠ¡å™¨: æ­£å¸¸")
            else:
                print(f"  âš ï¸  PrefillæœåŠ¡å™¨: çŠ¶æ€ç  {response.status_code}")
        except Exception as e:
            print(f"  âš ï¸  PrefillæœåŠ¡å™¨: æ— æ³•ç›´æ¥è¿æ¥ ({e})")
        
        # æ£€æŸ¥ Decode æœåŠ¡å™¨
        try:
            response = self.session.get(f"{self.decode_url}/health", timeout=5)
            if response.status_code == 200:
                print(f"  âœ… DecodeæœåŠ¡å™¨: æ­£å¸¸")
            else:
                print(f"  âš ï¸  DecodeæœåŠ¡å™¨: çŠ¶æ€ç  {response.status_code}")
        except Exception as e:
            print(f"  âš ï¸  DecodeæœåŠ¡å™¨: æ— æ³•ç›´æ¥è¿æ¥ ({e})")
        
        return all_healthy
    
    def generate_single(self, prompt, max_tokens=128, temperature=0.7, request_id=None):
        """å‘é€å•ä¸ªæµå¼ç”Ÿæˆè¯·æ±‚ï¼ˆé€šè¿‡ä»£ç†æœåŠ¡å™¨ï¼Œè‡ªåŠ¨åˆ†ç¦» prefill å’Œ decodeï¼‰"""
        payload = {
            "model": "/share-data/wzk-1/model/Qwen3-4B",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True
        }
        
        # ä¼°ç®—tokenæ•°é‡
        estimated_tokens = self.estimate_tokens(prompt)
        # ç«‹å³æ˜¾ç¤ºè¯·æ±‚å¼€å§‹
        print(f"\n[è¯·æ±‚ {request_id+1}] å¼€å§‹å¤„ç†ï¼ˆDisaggregatedæ¨¡å¼ï¼‰...")
        print(f"  æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        print(f"  ä¼°ç®—tokens: {estimated_tokens}")
        # åªæ˜¾ç¤ºåŸå§‹é—®é¢˜éƒ¨åˆ†ï¼Œä¸æ˜¾ç¤ºå¡«å……å†…å®¹
        original_prompt = prompt.split('\n\n')[0] if '\n\n' in prompt else prompt[:100]
        print(f"  é—®é¢˜: {original_prompt[:80]}{'...' if len(original_prompt) > 80 else ''}")
        print(f"  â†’ æµç¨‹: Prefillé˜¶æ®µ â†’ KV cacheä¼ è¾“ â†’ Decodeé˜¶æ®µ")
        print(f"  ç”Ÿæˆå†…å®¹: ", end="")
        sys.stdout.flush()
        
        try:
            start_time = time.time()
            first_token_time = None
            token_count = 0
            generated_text = ""
            token_times = []
            # æ ¹æ®max_tokensåŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
            timeout = max(30, max_tokens * 2)
            
            response = self.session.post(
                f"{self.proxy_url}/v1/completions",
                json=payload,
                timeout=timeout,
                stream=True
            )
            response.raise_for_status()
            
            # å¤„ç†æµå¼å“åº”
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data_str = line[6:]
                        if data_str.strip() == '[DONE]':
                            break
                        try:
                            data = json.loads(data_str)
                            choices = data.get('choices', [])
                            if choices:
                                choice = choices[0]
                                text_content = choice.get('text', '')
                                if text_content:
                                    current_time = time.time()
                                    token_count += 1
                                    
                                    # è®°å½•é¦–tokenæ—¶é—´
                                    if first_token_time is None:
                                        first_token_time = current_time - start_time
                                    
                                    # è®°å½•æ¯ä¸ªtokençš„æ—¶é—´
                                    token_times.append(current_time - start_time)
                                    
                                    generated_text += text_content
                                    
                                    # å®æ—¶æ˜¾ç¤ºtokenå†…å®¹å’Œæ—¶é—´
                                    display_text = text_content.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                                    if len(display_text) > 20:
                                        display_text = display_text[:20] + "..."
                                    print(f"    [è¯·æ±‚ {request_id+1}] Token {token_count}: '{display_text}' (æ—¶é—´: {current_time - start_time:.4f}s)")
                        except json.JSONDecodeError:
                            continue
            
            end_time = time.time()
            
            # è®¡ç®—æ—¶é—´æŒ‡æ ‡
            total_time = end_time - start_time
            ttft = first_token_time if first_token_time else 0
            
            print(f"\n[è¯·æ±‚ {request_id+1}] âœ… å®Œæˆ")
            print(f"  æ€»æ—¶é—´: {total_time:.3f}s")
            print(f"  TTFT (é¦–tokenå»¶è¿Ÿ): {ttft:.3f}s")
            print(f"  ç”Ÿæˆtokens: {token_count}")
            if token_count > 1:
                tpot = (total_time - ttft) / (token_count - 1) if token_count > 1 else 0
                print(f"  TPOT (æ¯tokenæ—¶é—´): {tpot*1000:.2f}ms")
            print("-" * 60)
            sys.stdout.flush()
            
            return {
                'request_id': request_id,
                'prompt': prompt,
                'generated_text': generated_text,
                'response_time': total_time,
                'ttft': ttft,
                'token_count': token_count,
                'token_times': token_times,
                'success': True,
                'error': None
            }
        except Exception as e:
            error_msg = str(e)
            print(f"\n[è¯·æ±‚ {request_id+1}] âŒ å¤±è´¥")
            print(f"  é”™è¯¯: {error_msg}")
            print("-" * 60)
            sys.stdout.flush()
            return {
                'request_id': request_id,
                'prompt': prompt,
                'generated_text': '',
                'response_time': 0,
                'ttft': 0,
                'token_count': 0,
                'token_times': [],
                'success': False,
                'error': error_msg
            }
    
    def generate_batch(self, prompts, max_tokens=64, temperature=0.7, batch_size=None):
        """æ‰¹é‡ç”Ÿæˆè¯·æ±‚ - æ‰¹æ¬¡æ•°å°±æ˜¯åŒæ—¶å‘é€çš„è¯·æ±‚æ•°"""
        if batch_size is None:
            batch_size = len(prompts)
            
        print(f"\n" + "="*80)
        print(f"å¼€å§‹ Disaggregated Prefill+Decode æ‰¹é‡æµ‹è¯•")
        print(f"="*80)
        print(f"æ¶æ„: Prefill å’Œ Decode åˆ†ç¦»")
        print(f"æ€»è¯·æ±‚æ•°: {len(prompts)}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size} (åŒæ—¶å‘é€çš„è¯·æ±‚æ•°)")
        print(f"æœ€å¤§tokens: {max_tokens}")
        print("=" * 80)
        
        # æ‰“å°æç¤ºè¯æ¦‚è§ˆ
        print("æç¤ºè¯æ¦‚è§ˆ:")
        for i, prompt in enumerate(prompts):
            estimated_tokens = self.estimate_tokens(prompt)
            print(f"  è¯·æ±‚ {i+1}: {len(prompt)} å­—ç¬¦, ~{estimated_tokens} tokens")
            # åªæ˜¾ç¤ºåŸå§‹é—®é¢˜éƒ¨åˆ†ï¼Œä¸æ˜¾ç¤ºå¡«å……å†…å®¹
            original_prompt = prompt.split('\n\n')[0] if '\n\n' in prompt else prompt[:100]
            print(f"    é—®é¢˜: {original_prompt[:60]}{'...' if len(original_prompt) > 60 else ''}")
        print("=" * 80)
        
        all_results = []
        start_time = time.time()
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for batch_start in range(0, len(prompts), batch_size):
            batch_end = min(batch_start + batch_size, len(prompts))
            batch_prompts = prompts[batch_start:batch_end]
            
            print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}: è¯·æ±‚ {batch_start+1}-{batch_end}")
            print("=" * 80)
            
            # ä½¿ç”¨ThreadPoolExecutorå¤„ç†å½“å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=batch_size) as executor:
                # æäº¤å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä»»åŠ¡
                future_to_prompt = {
                    executor.submit(self.generate_single, prompt, max_tokens, temperature, batch_start + i): prompt
                    for i, prompt in enumerate(batch_prompts)
                }
                
                # å®æ—¶ç­‰å¾…å¹¶æ˜¾ç¤ºç»“æœ
                batch_results = []
                completed_count = 0
                for future in as_completed(future_to_prompt):
                    result = future.result()
                    batch_results.append(result)
                    all_results.append(result)
                    completed_count += 1
                    
                    # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                    print(f"ğŸ“Š æ‰¹æ¬¡è¿›åº¦: {completed_count}/{len(batch_prompts)} å®Œæˆ")
                    sys.stdout.flush()
                
                # æ‰“å°æ‰¹æ¬¡ç»Ÿè®¡
                successful = sum(1 for r in batch_results if r['success'])
                failed = len(batch_results) - successful
                avg_time = sum(r['response_time'] for r in batch_results if r['success']) / max(successful, 1)
                avg_ttft = sum(r['ttft'] for r in batch_results if r['success']) / max(successful, 1)
                
                print(f"\næ‰¹æ¬¡ {batch_start//batch_size + 1} å®Œæˆ:")
                print(f"  æˆåŠŸ: {successful}/{len(batch_prompts)}")
                print(f"  å¤±è´¥: {failed}")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
                print(f"  å¹³å‡TTFT: {avg_ttft:.3f}s")
                print("=" * 80)
        
        total_time = time.time() - start_time
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        successful_results = [r for r in all_results if r['success']]
        failed_results = [r for r in all_results if not r['success']]
        
        if successful_results:
            response_times = [r['response_time'] for r in successful_results]
            ttft_times = [r['ttft'] for r in successful_results if 'ttft' in r]
            token_counts = [r['token_count'] for r in successful_results if 'token_count' in r]
            
            avg_response_time = sum(response_times) / len(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            throughput = len(successful_results) / total_time
            
            if ttft_times:
                avg_ttft = sum(ttft_times) / len(ttft_times)
                min_ttft = min(ttft_times)
                max_ttft = max(ttft_times)
            else:
                avg_ttft = min_ttft = max_ttft = 0
                
            if token_counts:
                avg_tokens = sum(token_counts) / len(token_counts)
                total_tokens = sum(token_counts)
                token_throughput = total_tokens / total_time
            else:
                avg_tokens = total_tokens = token_throughput = 0
        else:
            avg_response_time = min_response_time = max_response_time = 0
            avg_ttft = min_ttft = max_ttft = 0
            avg_tokens = total_tokens = token_throughput = 0
            throughput = 0
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print(f"\n" + "="*80)
        print(f"æœ€ç»ˆç»Ÿè®¡ (Disaggregated Prefill+Decode æ¶æ„)")
        print(f"="*80)
        print(f"ğŸ“Š è¯·æ±‚ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {len(prompts)}")
        print(f"  æˆåŠŸè¯·æ±‚: {len(successful_results)}")
        print(f"  å¤±è´¥è¯·æ±‚: {len(failed_results)}")
        print(f"  æˆåŠŸç‡: {len(successful_results)/len(prompts)*100:.1f}%")
        print(f"\nâ±ï¸  å»¶è¿Ÿç»Ÿè®¡:")
        print(f"  æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        print(f"  æœ€å°å“åº”æ—¶é—´: {min_response_time:.3f}s")
        print(f"  æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.3f}s")
        print(f"  å¹³å‡TTFT: {avg_ttft:.3f}s")
        print(f"  æœ€å°TTFT: {min_ttft:.3f}s")
        print(f"  æœ€å¤§TTFT: {max_ttft:.3f}s")
        print(f"\nğŸš€ ååé‡ç»Ÿè®¡:")
        print(f"  æ€»ç”Ÿæˆtokens: {total_tokens}")
        print(f"  å¹³å‡tokens/è¯·æ±‚: {avg_tokens:.1f}")
        print(f"  è¯·æ±‚ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"  Tokenååé‡: {token_throughput:.2f} tokens/ç§’")
        print(f"="*80)
        
        return {
            'architecture': 'disaggregated',
            'total_requests': len(prompts),
            'successful_requests': len(successful_results),
            'failed_requests': len(failed_results),
            'total_time': total_time,
            'avg_response_time': avg_response_time,
            'min_response_time': min_response_time,
            'max_response_time': max_response_time,
            'avg_ttft': avg_ttft,
            'min_ttft': min_ttft,
            'max_ttft': max_ttft,
            'total_tokens': total_tokens,
            'avg_tokens': avg_tokens,
            'throughput': throughput,
            'token_throughput': token_throughput,
            'results': all_results
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Disaggregated Prefill+Decode æ‰¹é‡æµ‹è¯•å·¥å…·')
    parser.add_argument('--batch-size', type=int, required=True, help='æ‰¹æ¬¡å¤§å°ï¼ˆåŒæ—¶å‘é€çš„è¯·æ±‚æ•°ï¼‰')
    parser.add_argument('--requests', type=int, default=8, help='æ€»è¯·æ±‚æ•°')
    parser.add_argument('--max-tokens', type=int, default=128, help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    parser.add_argument('--test-length', type=str, default='2048', 
                       choices=['1024', '2048', '4096', '8192'],
                       help='æµ‹è¯•é•¿åº¦ï¼ˆtokenæ•°ï¼‰ï¼š1024, 2048, 4096, 8192')
    parser.add_argument('--preset', type=str, choices=['short', 'medium', 'long'],
                       help='é¢„è®¾é…ç½®: short(1024), medium(2048), long(4096)')
    parser.add_argument('--proxy-url', type=str, default='http://localhost:8000', 
                       help='ä»£ç†æœåŠ¡å™¨URL')
    parser.add_argument('--prefill-url', type=str, default='http://localhost:8100',
                       help='PrefillæœåŠ¡å™¨URLï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰')
    parser.add_argument('--decode-url', type=str, default='http://localhost:8200',
                       help='DecodeæœåŠ¡å™¨URLï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰')
    parser.add_argument('--output-dir', type=str, default='disagg_batch_results', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # å¤„ç†é¢„è®¾é…ç½®
    if args.preset:
        preset_configs = {
            'short': {'test_length': '1024', 'max_tokens': 64, 'requests': 8},
            'medium': {'test_length': '2048', 'max_tokens': 128, 'requests': 8},
            'long': {'test_length': '4096', 'max_tokens': 256, 'requests': 4}
        }
        
        config = preset_configs[args.preset]
        args.test_length = config['test_length']
        args.max_tokens = config['max_tokens']
        args.requests = config['requests']
        
        print(f"ä½¿ç”¨é¢„è®¾é…ç½®: {args.preset}")
        print(f"  æµ‹è¯•é•¿åº¦: {args.test_length} tokens")
        print(f"  æœ€å¤§tokens: {args.max_tokens}")
        print(f"  æ€»è¯·æ±‚æ•°: {args.requests}")
    
    print("\n" + "="*80)
    print("Disaggregated Prefill+Decode æ‰¹é‡æµ‹è¯•å·¥å…·")
    print("="*80)
    print(f"æ¶æ„æ¨¡å¼: Prefill å’Œ Decode åˆ†ç¦»")
    print(f"ä»£ç†æœåŠ¡å™¨: {args.proxy_url}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"æ€»è¯·æ±‚æ•°: {args.requests}")
    print(f"æœ€å¤§tokens: {args.max_tokens}")
    print(f"æµ‹è¯•é•¿åº¦: {args.test_length} tokens")
    print("="*80)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = DisaggBatchClient(args.proxy_url, args.prefill_url, args.decode_url)
    
    # å¥åº·æ£€æŸ¥
    if not client.health_check():
        print("\nâš ï¸  è­¦å‘Š: éƒ¨åˆ†æœåŠ¡æœªå°±ç»ªï¼Œä½†ç»§ç»­æµ‹è¯•...")
    
    # ç”Ÿæˆæµ‹è¯•æç¤ºè¯
    base_prompts = [
        "1. è‹¥3å°æœºå™¨5å°æ—¶ç”Ÿäº§180ä¸ªé›¶ä»¶ï¼Œ7å°æœºå™¨8å°æ—¶å¯ç”Ÿäº§å¤šå°‘é›¶ä»¶ï¼Ÿ",
        "2. ç”²æ¯”ä¹™å¤§6å²ï¼Œ5å¹´å‰ç”²å¹´é¾„æ˜¯ä¹™çš„2å€ï¼Œæ±‚ä¸¤äººç°åœ¨å¹´é¾„ã€‚",
        "3. ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹",
        "4. è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆç°è±¡",
        "5. ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ",
        "6. å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
        "7. è§£é‡Šä»€ä¹ˆæ˜¯å¾®æœåŠ¡æ¶æ„",
        "8. ä»€ä¹ˆæ˜¯å®¹å™¨åŒ–æŠ€æœ¯ï¼Ÿ"
    ]
    
    def pad_prompt_to_length(prompt, target_length=2048):
        """å°†æç¤ºè¯å¡«å……åˆ°æŒ‡å®šé•¿åº¦ï¼ˆä»¥å­—ç¬¦æ•°ä¼°ç®—ï¼Œçº¦4ä¸ªå­—ç¬¦=1ä¸ªtokenï¼‰"""
        # ä¼°ç®—å½“å‰é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2è®¡ç®—ï¼Œè‹±æ–‡æŒ‰1è®¡ç®—ï¼‰
        current_length = sum(2 if '\u4e00' <= char <= '\u9fff' else 1 for char in prompt)
        target_chars = target_length * 4  # å‡è®¾1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦
        
        if current_length >= target_chars:
            return prompt
        
        # å¡«å……å†…å®¹
        padding_text = """

è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•é•¿åºåˆ—å¤„ç†èƒ½åŠ›çš„å¡«å……æ–‡æœ¬ã€‚åœ¨disaggregated prefill+decodeæ¶æ„ä¸­ï¼Œprefillé˜¶æ®µè´Ÿè´£å¤„ç†è¾“å…¥åºåˆ—å¹¶ç”ŸæˆKV cacheï¼Œç„¶åå°†KV cacheä¼ è¾“åˆ°decodeé˜¶æ®µè¿›è¡Œtokenç”Ÿæˆã€‚

è¿™ç§åˆ†ç¦»æ¶æ„çš„ä¼˜åŠ¿åœ¨äºï¼š1ï¼‰prefillå’Œdecodeå¯ä»¥ä½¿ç”¨ä¸åŒçš„GPUèµ„æºï¼›2ï¼‰å¯ä»¥ä¼˜åŒ–å„è‡ªçš„æ‰¹å¤„ç†ç­–ç•¥ï¼›3ï¼‰æé«˜æ•´ä½“èµ„æºåˆ©ç”¨ç‡ã€‚

åœ¨æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬ä¼šå…³æ³¨ï¼šprefillå»¶è¿Ÿã€KV cacheä¼ è¾“æ—¶é—´ã€decodeååé‡ç­‰å…³é”®æŒ‡æ ‡ã€‚è¿™äº›æŒ‡æ ‡å¯¹äºè¯„ä¼°disaggregatedæ¶æ„çš„æ€§èƒ½éå¸¸é‡è¦ã€‚

å¡«å……æ–‡æœ¬ç»§ç»­ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œdisaggregatedæ¶æ„ç‰¹åˆ«é€‚åˆå¤„ç†é•¿åºåˆ—è¾“å…¥çš„åœºæ™¯ã€‚å› ä¸ºprefillé˜¶æ®µå¯ä»¥ä¸“æ³¨äºå¹¶è¡Œå¤„ç†é•¿è¾“å…¥åºåˆ—ï¼Œè€Œdecodeé˜¶æ®µå¯ä»¥ä¼˜åŒ–è¿ç»­tokenç”Ÿæˆçš„æ•ˆç‡ã€‚

å¯¹äºä¸åŒçš„åºåˆ—é•¿åº¦ï¼Œç³»ç»Ÿçš„è¡Œä¸ºå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚è¾ƒçŸ­çš„åºåˆ—å¯èƒ½ä¸ä¼šå……åˆ†ä½“ç°åˆ†ç¦»æ¶æ„çš„ä¼˜åŠ¿ï¼Œè€Œè¾ƒé•¿çš„åºåˆ—åˆ™èƒ½æ›´å¥½åœ°å±•ç¤ºprefillå’Œdecodeåˆ†ç¦»å¸¦æ¥çš„æ€§èƒ½æå‡ã€‚

åœ¨æ€§èƒ½æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è§‚å¯Ÿï¼š1ï¼‰TTFTï¼ˆé¦–tokenå»¶è¿Ÿï¼‰æ˜¯å¦å—prefillå’ŒKVä¼ è¾“å½±å“ï¼›2ï¼‰åç»­tokenç”Ÿæˆé€Ÿåº¦æ˜¯å¦ç¨³å®šï¼›3ï¼‰å¤šè¯·æ±‚å¹¶å‘æ—¶çš„èµ„æºè°ƒåº¦æ•ˆç‡ã€‚

æµ‹è¯•æ•°æ®å¡«å……ï¼šä¸ºç¡®ä¿æµ‹è¯•çš„å…¨é¢æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒé•¿åº¦å’Œå¤æ‚åº¦çš„è¾“å…¥åºåˆ—ï¼Œä»¥è¯„ä¼°disaggregatedæ¶æ„åœ¨å„ç§åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚
"""
        
        # è®¡ç®—éœ€è¦æ·»åŠ çš„å¡«å……æ–‡æœ¬é•¿åº¦
        remaining_chars = target_chars - current_length
        if remaining_chars > 0:
            # é‡å¤å¡«å……æ–‡æœ¬ç›´åˆ°è¾¾åˆ°ç›®æ ‡é•¿åº¦
            padded_prompt = prompt
            while len(padded_prompt) < target_chars:
                padded_prompt += padding_text
            return padded_prompt[:target_chars]
        
        return prompt
    
    # å°†æ¯ä¸ªæç¤ºè¯å¡«å……åˆ°æŒ‡å®šé•¿åº¦
    test_length = int(args.test_length)
    padded_prompts = [pad_prompt_to_length(prompt, test_length) for prompt in base_prompts]
    
    # æ ¹æ®è¯·æ±‚æ•°é‡ç”Ÿæˆæç¤ºè¯
    test_prompts = (padded_prompts * ((args.requests // len(padded_prompts)) + 1))[:args.requests]
    
    # åˆ›å»ºå­˜å‚¨ç›®å½•
    storage_dir = Path(args.output_dir)
    storage_dir.mkdir(exist_ok=True)
    
    # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
    print(f"\nå¼€å§‹æ‰¹é‡æµ‹è¯•...")
    batch_stats = client.generate_batch(test_prompts, max_tokens=args.max_tokens, batch_size=args.batch_size)
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    result_file = storage_dir / f"batch_results_{timestamp}.json"
    
    with open(result_file, "w", encoding="utf-8") as f:
        json.dump(batch_stats, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ° {result_file}")

if __name__ == "__main__":
    main()

