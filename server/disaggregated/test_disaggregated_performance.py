#!/usr/bin/env python3
"""
VLLM åˆ†ç¦»å¼ Prefill+Decode æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹é‡ TTFTã€TBTã€E2E Latency å’Œè¯¦ç»†åŠŸè€—å˜åŒ–
"""

import requests
import json
import time
import threading
import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ gpu_monitor
sys.path.append(str(Path(__file__).parent.parent.parent))
from gpu_monitor import GPUMonitor

class DisaggregatedClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def health_check(self):
        """æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.base_url}/health")
            return response.status_code == 200
        except Exception as e:
            print(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def generate(self, prompt, max_tokens=16, temperature=0.7):
        """å‘é€ç”Ÿæˆè¯·æ±‚åˆ°åˆ†ç¦»å¼æœåŠ¡å™¨"""
        payload = {
            "model": "/share-data/wzk-1/model/deepseek-v2-lite",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        try:
            response = self.session.post(
                f"{self.base_url}/v1/completions",
                json=payload,
                timeout=120
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"ç”Ÿæˆè¯·æ±‚å¤±è´¥: {e}")
            return None

def test_disaggregated_performance(client, prompts, gpu_monitor_prefill, gpu_monitor_decode):
    """æµ‹è¯•åˆ†ç¦»å¼ prefill+decode æ€§èƒ½"""
    print("\n=== åˆ†ç¦»å¼ Prefill+Decode æ€§èƒ½æµ‹è¯• ===")
    
    results = []
    
    for i, prompt in enumerate(prompts):
        print(f"\nå¤„ç†è¯·æ±‚ {i+1}/{len(prompts)}: {prompt[:50]}...")
        
        # å¯åŠ¨ GPU ç›‘æ§
        gpu_monitor_prefill.start_monitoring()
        gpu_monitor_decode.start_monitoring()
        
        # è®°å½• E2E å¼€å§‹æ—¶é—´
        e2e_start = time.time()
        
        # å‘é€è¯·æ±‚
        result = client.generate(prompt, max_tokens=16)
        
        # è®°å½• E2E ç»“æŸæ—¶é—´
        e2e_end = time.time()
        e2e_latency = e2e_end - e2e_start
        
        # åœæ­¢ GPU ç›‘æ§
        gpu_monitor_prefill.stop_monitoring()
        gpu_monitor_decode.stop_monitoring()
        
        if result:
            # æå–æ—¶é—´æŒ‡æ ‡
            timing = result.get("timing", {})
            usage = result.get("usage", {})
            choices = result.get("choices", [])
            generated_text = choices[0].get("text", "") if choices else ""
            
            # è®¡ç®—æŒ‡æ ‡
            ttft = timing.get("prefill_time", 0)  # Time to First Token
            tbt = timing.get("tbt", 0)  # Time Between Tokens
            prefill_time = timing.get("prefill_time", 0)
            decode_time = timing.get("decode_time", 0)
            
            result_data = {
                "request_id": i,
                "prompt": prompt,
                "generated_text": generated_text,
                "e2e_latency": e2e_latency,
                "ttft": ttft,
                "tbt": tbt,
                "prefill_time": prefill_time,
                "decode_time": decode_time,
                "prompt_tokens": usage.get("prompt_tokens", 0),
                "completion_tokens": usage.get("completion_tokens", 0),
                "total_tokens": usage.get("total_tokens", 0),
                "result": result
            }
            
            results.append(result_data)
            
            print(f"  âœ… è¯·æ±‚å®Œæˆ")
            print(f"     E2E Latency: {e2e_latency:.4f}s")
            print(f"     TTFT: {ttft:.4f}s")
            print(f"     TBT: {tbt:.4f}s")
            print(f"     Prefill: {prefill_time:.4f}s")
            print(f"     Decode: {decode_time:.4f}s")
            print(f"     Tokens: {usage.get('completion_tokens', 0)}")
            print(f"     ç”Ÿæˆå†…å®¹: {generated_text[:50]}...")
        else:
            print(f"  âŒ è¯·æ±‚å¤±è´¥")
    
    return results

def create_detailed_power_visualization(prefill_gpu_data, decode_gpu_data, results, storage_dir):
    """åˆ›å»ºè¯¦ç»†çš„åŠŸè€—å˜åŒ–å›¾è¡¨"""
    
    # åˆ›å»ºå›¾è¡¨
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Disaggregated Prefill+Decode Performance Analysis', fontsize=16, fontweight='bold')
    
    # 1. åŠŸè€—æ—¶é—´åºåˆ—å›¾
    if prefill_gpu_data and 'raw_data' in prefill_gpu_data:
        prefill_timestamps = [d['timestamp'] for d in prefill_gpu_data['raw_data']]
        prefill_power = [d['power_draw'] for d in prefill_gpu_data['raw_data']]
        prefill_times = [(t - prefill_timestamps[0]) for t in prefill_timestamps]
        
        ax1.plot(prefill_times, prefill_power, 'b-', label='Prefill GPU Power', linewidth=2)
    
    if decode_gpu_data and 'raw_data' in decode_gpu_data:
        decode_timestamps = [d['timestamp'] for d in decode_gpu_data['raw_data']]
        decode_power = [d['power_draw'] for d in decode_gpu_data['raw_data']]
        decode_times = [(t - decode_timestamps[0]) for t in decode_timestamps]
        
        ax1.plot(decode_times, decode_power, 'r-', label='Decode GPU Power', linewidth=2)
    
    ax1.set_xlabel('Time (s)')
    ax1.set_ylabel('Power (W)')
    ax1.set_title('Power Consumption Over Time')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. é¢‘ç‡å˜åŒ–å›¾
    if prefill_gpu_data and 'raw_data' in prefill_gpu_data:
        prefill_freq = [d['graphics_clock'] for d in prefill_gpu_data['raw_data']]
        ax2.plot(prefill_times, prefill_freq, 'b-', label='Prefill GPU Freq', linewidth=2)
    
    if decode_gpu_data and 'raw_data' in decode_gpu_data:
        decode_freq = [d['graphics_clock'] for d in decode_gpu_data['raw_data']]
        ax2.plot(decode_times, decode_freq, 'r-', label='Decode GPU Freq', linewidth=2)
    
    ax2.set_xlabel('Time (s)')
    ax2.set_ylabel('Frequency (MHz)')
    ax2.set_title('GPU Frequency Over Time')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    if results:
        ttft_values = [r['ttft'] for r in results]
        tbt_values = [r['tbt'] for r in results]
        e2e_values = [r['e2e_latency'] for r in results]
        
        x = np.arange(len(results))
        width = 0.25
        
        ax3.bar(x - width, ttft_values, width, label='TTFT', color='skyblue', alpha=0.8)
        ax3.bar(x, tbt_values, width, label='TBT', color='lightcoral', alpha=0.8)
        ax3.bar(x + width, e2e_values, width, label='E2E Latency', color='lightgreen', alpha=0.8)
        
        ax3.set_xlabel('Request ID')
        ax3.set_ylabel('Time (s)')
        ax3.set_title('Performance Metrics by Request')
        ax3.set_xticks(x)
        ax3.set_xticklabels([f'Req {i+1}' for i in range(len(results))])
        ax3.legend()
        ax3.grid(True, alpha=0.3)
    
    # 4. åŠŸè€—ç»Ÿè®¡å¯¹æ¯”
    if prefill_gpu_data and decode_gpu_data:
        prefill_stats = prefill_gpu_data.get('statistics', {})
        decode_stats = decode_gpu_data.get('statistics', {})
        
        categories = ['Avg Power', 'Max Power', 'Avg Freq', 'Max Freq']
        prefill_values = [
            prefill_stats.get('power_draw', {}).get('avg', 0),
            prefill_stats.get('power_draw', {}).get('max', 0),
            prefill_stats.get('graphics_clock', {}).get('avg', 0),
            prefill_stats.get('graphics_clock', {}).get('max', 0)
        ]
        decode_values = [
            decode_stats.get('power_draw', {}).get('avg', 0),
            decode_stats.get('power_draw', {}).get('max', 0),
            decode_stats.get('graphics_clock', {}).get('avg', 0),
            decode_stats.get('graphics_clock', {}).get('max', 0)
        ]
        
        x4 = np.arange(len(categories))
        width = 0.35
        
        ax4.bar(x4 - width/2, prefill_values, width, label='Prefill GPU', color='blue', alpha=0.8)
        ax4.bar(x4 + width/2, decode_values, width, label='Decode GPU', color='red', alpha=0.8)
        
        ax4.set_xlabel('Metrics')
        ax4.set_ylabel('Value')
        ax4.set_title('GPU Performance Comparison')
        ax4.set_xticks(x4)
        ax4.set_xticklabels(categories, rotation=45)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(storage_dir / "disaggregated_performance_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š è¯¦ç»†æ€§èƒ½åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {storage_dir / 'disaggregated_performance_analysis.png'}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("VLLM åˆ†ç¦»å¼ Prefill+Decode æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = DisaggregatedClient("http://localhost:8000")
    
    # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
    print("æ£€æŸ¥åˆ†ç¦»å¼æœåŠ¡å™¨çŠ¶æ€...")
    if not client.health_check():
        print("âŒ åˆ†ç¦»å¼æœåŠ¡å™¨æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡å™¨")
        print("è¿è¡Œå‘½ä»¤: ./start_disaggregated_servers.sh")
        return
    
    print("âœ… åˆ†ç¦»å¼æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
    
    # æµ‹è¯•æç¤ºè¯
    test_prompts = [
        "1. è‹¥3å°æœºå™¨5å°æ—¶ç”Ÿäº§180ä¸ªé›¶ä»¶ï¼Œ7å°æœºå™¨8å°æ—¶å¯ç”Ÿäº§å¤šå°‘é›¶ä»¶ï¼Ÿ",
        "2. ç”²æ¯”ä¹™å¤§6å²ï¼Œ5å¹´å‰ç”²å¹´é¾„æ˜¯ä¹™çš„2å€ï¼Œæ±‚ä¸¤äººç°åœ¨å¹´é¾„ã€‚",
        "3. ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹",
        "4. è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆç°è±¡"
    ]
    
    # åˆ›å»ºå­˜å‚¨ç›®å½•
    storage_dir = Path("disaggregated_results")
    storage_dir.mkdir(exist_ok=True)
    
    # åˆå§‹åŒ– GPU ç›‘æ§å™¨ï¼ˆåˆ†åˆ«ç›‘æ§ä¸¤ä¸ª GPUï¼‰
    gpu_monitor_prefill = GPUMonitor(gpu_id=0, interval=0.1)  # GPU 0 (Prefill)
    gpu_monitor_decode = GPUMonitor(gpu_id=1, interval=0.1)   # GPU 1 (Decode)
    
    # æ‰§è¡Œåˆ†ç¦»å¼æ€§èƒ½æµ‹è¯•
    print(f"\nå¼€å§‹åˆ†ç¦»å¼æ€§èƒ½æµ‹è¯•ï¼Œå¤„ç† {len(test_prompts)} ä¸ªè¯·æ±‚...")
    results = test_disaggregated_performance(client, test_prompts, gpu_monitor_prefill, gpu_monitor_decode)
    
    # è·å– GPU ç›‘æ§æ•°æ®
    prefill_gpu_stats = gpu_monitor_prefill.get_statistics()
    decode_gpu_stats = gpu_monitor_decode.get_statistics()
    
    # ä¿å­˜ç»“æœ
    test_results = {
        "test_type": "disaggregated_prefill_decode",
        "total_requests": len(results),
        "results": results,
        "prefill_gpu_statistics": prefill_gpu_stats,
        "decode_gpu_statistics": decode_gpu_stats,
        "summary": {
            "avg_e2e_latency": np.mean([r['e2e_latency'] for r in results]) if results else 0,
            "avg_ttft": np.mean([r['ttft'] for r in results]) if results else 0,
            "avg_tbt": np.mean([r['tbt'] for r in results]) if results else 0,
            "avg_prefill_time": np.mean([r['prefill_time'] for r in results]) if results else 0,
            "avg_decode_time": np.mean([r['decode_time'] for r in results]) if results else 0
        }
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(storage_dir / "disaggregated_results.json", "w", encoding="utf-8") as f:
        json.dump(test_results, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ GPU ç›‘æ§æ•°æ®
    gpu_monitor_prefill.save_data(storage_dir / "prefill_gpu_data.json")
    gpu_monitor_decode.save_data(storage_dir / "decode_gpu_data.json")
    
    # åˆ›å»ºè¯¦ç»†çš„å¯è§†åŒ–å›¾è¡¨
    prefill_gpu_data = json.load(open(storage_dir / "prefill_gpu_data.json", "r"))
    decode_gpu_data = json.load(open(storage_dir / "decode_gpu_data.json", "r"))
    create_detailed_power_visualization(prefill_gpu_data, decode_gpu_data, results, storage_dir)
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("åˆ†ç¦»å¼æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print(f"æ€»è¯·æ±‚æ•°: {len(results)}")
    
    if results:
        summary = test_results["summary"]
        print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡æ€»ç»“:")
        print(f"  å¹³å‡ E2E Latency: {summary['avg_e2e_latency']:.4f}s")
        print(f"  å¹³å‡ TTFT: {summary['avg_ttft']:.4f}s")
        print(f"  å¹³å‡ TBT: {summary['avg_tbt']:.4f}s")
        print(f"  å¹³å‡ Prefill æ—¶é—´: {summary['avg_prefill_time']:.4f}s")
        print(f"  å¹³å‡ Decode æ—¶é—´: {summary['avg_decode_time']:.4f}s")
        
        print(f"\nâš¡ GPU åŠŸè€—æ€»ç»“:")
        if prefill_gpu_stats:
            print(f"  Prefill GPU å¹³å‡åŠŸè€—: {prefill_gpu_stats['power_draw']['avg']:.1f}W")
            print(f"  Prefill GPU æœ€å¤§åŠŸè€—: {prefill_gpu_stats['power_draw']['max']:.1f}W")
        if decode_gpu_stats:
            print(f"  Decode GPU å¹³å‡åŠŸè€—: {decode_gpu_stats['power_draw']['avg']:.1f}W")
            print(f"  Decode GPU æœ€å¤§åŠŸè€—: {decode_gpu_stats['power_draw']['max']:.1f}W")
    
    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ° {storage_dir} ç›®å½•")

if __name__ == "__main__":
    main()
