#!/usr/bin/env python3
"""
ç®€åŒ–çš„VLLMæ‰¹é‡æµ‹è¯•è„šæœ¬
åªä¿ç•™æ‰¹æ¬¡è°ƒèŠ‚åŠŸèƒ½ï¼Œæ‰¹æ¬¡æ•°å°±æ˜¯åŒæ—¶å‘é€çš„è¯·æ±‚æ•°
"""

import requests
import json
import time
import threading
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ gpu_monitor
sys.path.append(str(Path(__file__).parent.parent.parent))
from gpu_monitor import GPUMonitor

class SimpleBatchClient:
    """ç®€åŒ–çš„æ‰¹é‡æµ‹è¯•å®¢æˆ·ç«¯"""
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
    def estimate_tokens(self, text):
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2è®¡ç®—ï¼Œè‹±æ–‡æŒ‰1è®¡ç®—ï¼‰"""
        return sum(2 if '\u4e00' <= char <= '\u9fff' else 1 for char in text)
    
    def health_check(self):
        """æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.base_url}/health")
            return response.status_code == 200
        except Exception as e:
            print(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def generate_single(self, prompt, max_tokens=128, temperature=0.7, request_id=None):
        """å‘é€å•ä¸ªæµå¼ç”Ÿæˆè¯·æ±‚"""
        payload = {
            "model": "/share-data/wzk-1/model/Qwen3-8B",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True
        }
        
        # ä¼°ç®—tokenæ•°é‡
        estimated_tokens = self.estimate_tokens(prompt)
        # ç«‹å³æ˜¾ç¤ºè¯·æ±‚å¼€å§‹
        print(f"\n[è¯·æ±‚ {request_id+1}] å¼€å§‹å¤„ç†...")
        print(f"  æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        print(f"  ä¼°ç®—tokens: {estimated_tokens}")
        # åªæ˜¾ç¤ºåŸå§‹é—®é¢˜éƒ¨åˆ†ï¼Œä¸æ˜¾ç¤ºå¡«å……å†…å®¹
        original_prompt = prompt.split('\n\n')[0] if '\n\n' in prompt else prompt[:100]
        print(f"  é—®é¢˜: {original_prompt[:80]}{'...' if len(original_prompt) > 80 else ''}")
        print(f"  ç”Ÿæˆå†…å®¹: ", end="")
        sys.stdout.flush()
        try:
            start_time = time.time()
            first_token_time = None
            token_count = 0
            generated_text = ""
            token_times = []
            # æ ¹æ®max_tokensåŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
            timeout = max(30, max_tokens * 2)
            response = self.session.post(
                f"{self.base_url}/v1/completions",
                json=payload,
                timeout=timeout,
                stream=True
            )
            response.raise_for_status()
            # å¤„ç†æµå¼å“åº”
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data_str = line[6:]
                        if data_str.strip() == '[DONE]':
                            break
                        try:
                            data = json.loads(data_str)
                            choices = data.get('choices', [])
                            if choices:
                                choice = choices[0]
                                text_content = choice.get('text', '')
                                if text_content:
                                    current_time = time.time()
                                    token_count += 1
                                    
                                    # è®°å½•é¦–tokenæ—¶é—´
                                    if first_token_time is None:
                                        first_token_time = current_time - start_time
                                    
                                    # è®°å½•æ¯ä¸ªtokençš„æ—¶é—´
                                    token_times.append(current_time - start_time)
                                    
                                    generated_text += text_content
                                    
                                    # å®æ—¶æ˜¾ç¤ºtokenå†…å®¹å’Œæ—¶é—´ï¼Œæ ¼å¼ç±»ä¼¼test_vllm_unified.py
                                    display_text = text_content.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                                    if len(display_text) > 20:
                                        display_text = display_text[:20] + "..."
                                    print(f"    [è¯·æ±‚ {request_id+1}] Token {token_count}: '{display_text}' (æ—¶é—´: {current_time - start_time:.4f}s)")
                        except json.JSONDecodeError:
                            continue
            
            end_time = time.time()
            
            # è®¡ç®—æ—¶é—´æŒ‡æ ‡
            total_time = end_time - start_time
            ttft = first_token_time if first_token_time else 0
            
            print(f"\n[è¯·æ±‚ {request_id+1}] âœ… å®Œæˆ")
            print(f"  æ€»æ—¶é—´: {total_time:.3f}s")
            print(f"  TTFT: {ttft:.3f}s")
            print(f"  ç”Ÿæˆtokens: {token_count}")
            print("-" * 60)
            sys.stdout.flush()
            
            return {
                'request_id': request_id,
                'prompt': prompt,
                'generated_text': generated_text,
                'response_time': total_time,
                'ttft': ttft,
                'token_count': token_count,
                'token_times': token_times,
                'success': True,
                'error': None
            }
        except Exception as e:
            error_msg = str(e)
            print(f"\n[è¯·æ±‚ {request_id+1}] âŒ å¤±è´¥")
            print(f"  é”™è¯¯: {error_msg}")
            print("-" * 60)
            sys.stdout.flush()
            return {
                'request_id': request_id,
                'prompt': prompt,
                'generated_text': '',
                'response_time': 0,
                'ttft': 0,
                'token_count': 0,
                'token_times': [],
                'success': False,
                'error': error_msg
            }
    
    def generate_batch(self, prompts, max_tokens=64, temperature=0.7, batch_size=None):
        """æ‰¹é‡ç”Ÿæˆè¯·æ±‚ - æ‰¹æ¬¡æ•°å°±æ˜¯åŒæ—¶å‘é€çš„è¯·æ±‚æ•°"""
        if batch_size is None:
            batch_size = len(prompts)
        
        print(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆæµ‹è¯•")
        print(f"æ€»è¯·æ±‚æ•°: {len(prompts)}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size} (åŒæ—¶å‘é€çš„è¯·æ±‚æ•°)")
        print(f"æœ€å¤§tokens: {max_tokens}")
        print("=" * 60)
        
        # æ‰“å°æç¤ºè¯æ¦‚è§ˆ
        print("æç¤ºè¯æ¦‚è§ˆ:")
        for i, prompt in enumerate(prompts):
            estimated_tokens = self.estimate_tokens(prompt)
            print(f"  è¯·æ±‚ {i+1}: {len(prompt)} å­—ç¬¦, ~{estimated_tokens} tokens")
            # åªæ˜¾ç¤ºåŸå§‹é—®é¢˜éƒ¨åˆ†ï¼Œä¸æ˜¾ç¤ºå¡«å……å†…å®¹
            original_prompt = prompt.split('\n\n')[0] if '\n\n' in prompt else prompt[:100]
            print(f"    é—®é¢˜: {original_prompt[:60]}{'...' if len(original_prompt) > 60 else ''}")
        print("=" * 60)
        
        all_results = []
        start_time = time.time()
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for batch_start in range(0, len(prompts), batch_size):
            batch_end = min(batch_start + batch_size, len(prompts))
            batch_prompts = prompts[batch_start:batch_end]
            
            print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}: è¯·æ±‚ {batch_start+1}-{batch_end}")
            print("=" * 60)
            
            # ä½¿ç”¨ThreadPoolExecutorå¤„ç†å½“å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=batch_size) as executor:
                # æäº¤å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä»»åŠ¡
                future_to_prompt = {
                    executor.submit(self.generate_single, prompt, max_tokens, temperature, batch_start + i): prompt
                    for i, prompt in enumerate(batch_prompts)
                }
                
                # å®æ—¶ç­‰å¾…å¹¶æ˜¾ç¤ºç»“æœ
                batch_results = []
                completed_count = 0
                for future in as_completed(future_to_prompt):
                    result = future.result()
                    batch_results.append(result)
                    all_results.append(result)
                    completed_count += 1
                    
                    # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                    print(f"ğŸ“Š æ‰¹æ¬¡è¿›åº¦: {completed_count}/{len(batch_prompts)} å®Œæˆ")
                    sys.stdout.flush()
                
                # æ‰“å°æ‰¹æ¬¡ç»Ÿè®¡
                successful = sum(1 for r in batch_results if r['success'])
                failed = len(batch_results) - successful
                avg_time = sum(r['response_time'] for r in batch_results if r['success']) / max(successful, 1)
                
                print(f"\næ‰¹æ¬¡ {batch_start//batch_size + 1} å®Œæˆ:")
                print(f"  æˆåŠŸ: {successful}/{len(batch_prompts)}")
                print(f"  å¤±è´¥: {failed}")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
                print("=" * 60)
        
        total_time = time.time() - start_time
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        successful_results = [r for r in all_results if r['success']]
        failed_results = [r for r in all_results if not r['success']]
        
        if successful_results:
            response_times = [r['response_time'] for r in successful_results]
            ttft_times = [r['ttft'] for r in successful_results if 'ttft' in r]
            token_counts = [r['token_count'] for r in successful_results if 'token_count' in r]
            
            avg_response_time = sum(response_times) / len(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            throughput = len(successful_results) / total_time
            
            if ttft_times:
                avg_ttft = sum(ttft_times) / len(ttft_times)
                min_ttft = min(ttft_times)
                max_ttft = max(ttft_times)
            else:
                avg_ttft = min_ttft = max_ttft = 0
                
            if token_counts:
                avg_tokens = sum(token_counts) / len(token_counts)
                total_tokens = sum(token_counts)
            else:
                avg_tokens = total_tokens = 0
        else:
            avg_response_time = min_response_time = max_response_time = 0
            avg_ttft = min_ttft = max_ttft = 0
            avg_tokens = total_tokens = 0
            throughput = 0
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print(f"\næœ€ç»ˆç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {len(prompts)}")
        print(f"  æˆåŠŸè¯·æ±‚: {len(successful_results)}")
        print(f"  å¤±è´¥è¯·æ±‚: {len(failed_results)}")
        print(f"  æˆåŠŸç‡: {len(successful_results)/len(prompts)*100:.1f}%")
        print(f"  æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        print(f"  æœ€å°å“åº”æ—¶é—´: {min_response_time:.3f}s")
        print(f"  æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.3f}s")
        print(f"  å¹³å‡TTFT: {avg_ttft:.3f}s")
        print(f"  æœ€å°TTFT: {min_ttft:.3f}s")
        print(f"  æœ€å¤§TTFT: {max_ttft:.3f}s")
        print(f"  æ€»ç”Ÿæˆtokens: {total_tokens}")
        print(f"  å¹³å‡tokens/è¯·æ±‚: {avg_tokens:.1f}")
        print(f"  ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        
        return {
            'total_requests': len(prompts),
            'successful_requests': len(successful_results),
            'failed_requests': len(failed_results),
            'total_time': total_time,
            'avg_response_time': avg_response_time,
            'min_response_time': min_response_time,
            'max_response_time': max_response_time,
            'throughput': throughput,
            'results': all_results
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç®€åŒ–çš„VLLMæ‰¹é‡æµ‹è¯•å·¥å…·')
    parser.add_argument('--batch-size', type=int, required=True, help='æ‰¹æ¬¡å¤§å°ï¼ˆåŒæ—¶å‘é€çš„è¯·æ±‚æ•°ï¼‰')
    parser.add_argument('--requests', type=int, default=8, help='æ€»è¯·æ±‚æ•°')
    parser.add_argument('--max-tokens', type=int, default=128, help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    parser.add_argument('--test-length', type=str, default='4096', 
                       choices=['1024', '2048', '4096', '8192', '16384', '32768'],
                       help='æµ‹è¯•é•¿åº¦ï¼ˆtokenæ•°ï¼‰ï¼š1024, 2048, 4096, 8192, 16384, 32768')
    parser.add_argument('--preset', type=str, choices=['short', 'medium', 'long', 'xlong'],
                       help='é¢„è®¾é…ç½®: short(1024), medium(4096), long(8192), xlong(16384)')
    parser.add_argument('--output-dir', type=str, default='simple_batch_results', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # å¤„ç†é¢„è®¾é…ç½®
    if args.preset:
        preset_configs = {
            'short': {'test_length': '1024', 'max_tokens': 64, 'requests': 8},
            'medium': {'test_length': '4096', 'max_tokens': 128, 'requests': 8},
            'long': {'test_length': '8192', 'max_tokens': 256, 'requests': 4},
            'xlong': {'test_length': '16384', 'max_tokens': 512, 'requests': 2}
        }
        
        config = preset_configs[args.preset]
        args.test_length = config['test_length']
        args.max_tokens = config['max_tokens']
        args.requests = config['requests']
        
        print(f"ä½¿ç”¨é¢„è®¾é…ç½®: {args.preset}")
        print(f"  æµ‹è¯•é•¿åº¦: {args.test_length} tokens")
        print(f"  æœ€å¤§tokens: {args.max_tokens}")
        print(f"  æ€»è¯·æ±‚æ•°: {args.requests}")
    
    print("ç®€åŒ–çš„VLLMæ‰¹é‡æµ‹è¯•å·¥å…·")
    print("=" * 50)
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"æ€»è¯·æ±‚æ•°: {args.requests}")
    print(f"æœ€å¤§tokens: {args.max_tokens}")
    print(f"æµ‹è¯•é•¿åº¦: {args.test_length} tokens")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = SimpleBatchClient("http://localhost:8000")
      
    # ç”Ÿæˆæµ‹è¯•æç¤ºè¯
    base_prompts = [
        "1. è‹¥3å°æœºå™¨5å°æ—¶ç”Ÿäº§180ä¸ªé›¶ä»¶ï¼Œ7å°æœºå™¨8å°æ—¶å¯ç”Ÿäº§å¤šå°‘é›¶ä»¶ï¼Ÿ",
        "2. ç”²æ¯”ä¹™å¤§6å²ï¼Œ5å¹´å‰ç”²å¹´é¾„æ˜¯ä¹™çš„2å€ï¼Œæ±‚ä¸¤äººç°åœ¨å¹´é¾„ã€‚",
        "3. ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹",
        "4. è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆç°è±¡",
        "5. ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ",
        "6. å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
        "7. è§£é‡Šä»€ä¹ˆæ˜¯å¾®æœåŠ¡æ¶æ„",
        "8. ä»€ä¹ˆæ˜¯å®¹å™¨åŒ–æŠ€æœ¯ï¼Ÿ"
    ]
    
    def pad_prompt_to_length(prompt, target_length=4096):
        """å°†æç¤ºè¯å¡«å……åˆ°æŒ‡å®šé•¿åº¦ï¼ˆä»¥å­—ç¬¦æ•°ä¼°ç®—ï¼Œçº¦4ä¸ªå­—ç¬¦=1ä¸ªtokenï¼‰"""
        # ä¼°ç®—å½“å‰é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2è®¡ç®—ï¼Œè‹±æ–‡æŒ‰1è®¡ç®—ï¼‰
        current_length = sum(2 if '\u4e00' <= char <= '\u9fff' else 1 for char in prompt)
        target_chars = target_length * 4  # å‡è®¾1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦
        
        if current_length >= target_chars:
            return prompt
        
        # å¡«å……å†…å®¹
        padding_text = """
        
        è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•é•¿åºåˆ—å¤„ç†èƒ½åŠ›çš„å¡«å……æ–‡æœ¬ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œåºåˆ—é•¿åº¦æ˜¯ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼Œå®ƒç›´æ¥å½±å“æ¨¡å‹çš„æ€§èƒ½å’Œè®¡ç®—èµ„æºæ¶ˆè€—ã€‚è¾ƒé•¿çš„åºåˆ—å¯ä»¥åŒ…å«æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’Œå†…å­˜ã€‚

        åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œç‰¹åˆ«æ˜¯Transformeræ¶æ„ä¸­ï¼Œåºåˆ—é•¿åº¦çš„å¹³æ–¹å…³ç³»ä½¿å¾—é•¿åºåˆ—å¤„ç†å˜å¾—è®¡ç®—å¯†é›†ã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å¹³è¡¡åºåˆ—é•¿åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚

        å¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œé•¿åºåˆ—å¤„ç†èƒ½åŠ›æ˜¯è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„é‡è¦æŒ‡æ ‡ä¹‹ä¸€ã€‚æ¨¡å‹éœ€è¦èƒ½å¤Ÿç†è§£å’Œå¤„ç†é•¿è·ç¦»çš„ä¾èµ–å…³ç³»ï¼Œè¿™å¯¹äºè®¸å¤šå¤æ‚çš„è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡è‡³å…³é‡è¦ã€‚

        åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šç›‘æ§GPUçš„åŠŸè€—ã€é¢‘ç‡å’Œåˆ©ç”¨ç‡ï¼Œä»¥äº†è§£ä¸åŒåºåˆ—é•¿åº¦å¯¹ç¡¬ä»¶èµ„æºæ¶ˆè€—çš„å½±å“ã€‚è¿™å¯¹äºä¼˜åŒ–æ¨¡å‹éƒ¨ç½²å’Œèµ„æºåˆ†é…å…·æœ‰é‡è¦æ„ä¹‰ã€‚

        å¡«å……æ–‡æœ¬ç»§ç»­ï¼šåœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å®é™…åº”ç”¨ä¸­ï¼Œåºåˆ—é•¿åº¦æ˜¯ä¸€ä¸ªéœ€è¦ä»”ç»†è€ƒè™‘çš„è¶…å‚æ•°ã€‚ä¸åŒçš„ä»»åŠ¡å¯èƒ½éœ€è¦ä¸åŒçš„åºåˆ—é•¿åº¦ï¼Œè€Œæ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ä¹Ÿå—åˆ°ç¡¬ä»¶é™åˆ¶çš„çº¦æŸã€‚

        å¯¹äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œè¾ƒé•¿çš„è¾“å…¥åºåˆ—å¯ä»¥æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ‰åŠ©äºç”Ÿæˆæ›´ç›¸å…³å’Œè¿è´¯çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿæ„å‘³ç€éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’Œæ›´é•¿çš„å¤„ç†æ—¶é—´ã€‚

        åœ¨åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†ä¸­ï¼Œåºåˆ—é•¿åº¦çš„é€‰æ‹©è¿˜ä¼šå½±å“é€šä¿¡å¼€é”€å’Œå†…å­˜ä½¿ç”¨æ¨¡å¼ã€‚å› æ­¤ï¼Œåœ¨å®é™…éƒ¨ç½²ä¸­éœ€è¦æ ¹æ®å…·ä½“çš„ç¡¬ä»¶é…ç½®å’Œæ€§èƒ½è¦æ±‚æ¥é€‰æ‹©åˆé€‚çš„åºåˆ—é•¿åº¦ã€‚

        æµ‹è¯•æ•°æ®å¡«å……ï¼šä¸ºäº†ç¡®ä¿æµ‹è¯•çš„å…¨é¢æ€§ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸åŒé•¿åº¦å’Œå¤æ‚åº¦çš„è¾“å…¥åºåˆ—ã€‚è¿™åŒ…æ‹¬çŸ­åºåˆ—ã€ä¸­ç­‰é•¿åº¦åºåˆ—å’Œé•¿åºåˆ—ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨å„ç§æƒ…å†µä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

        åœ¨æ€§èƒ½æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨çš„ä¸»è¦æŒ‡æ ‡åŒ…æ‹¬ï¼šå»¶è¿Ÿï¼ˆlatencyï¼‰ã€ååé‡ï¼ˆthroughputï¼‰ã€å†…å­˜ä½¿ç”¨é‡ã€GPUåˆ©ç”¨ç‡ç­‰ã€‚è¿™äº›æŒ‡æ ‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬äº†è§£æ¨¡å‹åœ¨ä¸åŒè´Ÿè½½ä¸‹çš„è¡¨ç°ã€‚

        åºåˆ—é•¿åº¦çš„å½±å“ï¼šè¾ƒé•¿çš„åºåˆ—é€šå¸¸éœ€è¦æ›´å¤šçš„æ³¨æ„åŠ›è®¡ç®—ï¼Œè¿™ä¼šå¯¼è‡´äºŒæ¬¡æ–¹çš„æ—¶é—´å¤æ‚åº¦ã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚

        å¯¹äºé¢„å¡«å……ï¼ˆprefillï¼‰é˜¶æ®µï¼Œé•¿åºåˆ—æ„å‘³ç€éœ€è¦å¤„ç†æ›´å¤šçš„è¾“å…¥tokenï¼Œè¿™é€šå¸¸éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚è€Œå¯¹äºè§£ç ï¼ˆdecodeï¼‰é˜¶æ®µï¼Œåºåˆ—é•¿åº¦ä¸»è¦å½±å“KVç¼“å­˜çš„å­˜å‚¨éœ€æ±‚ã€‚

        åœ¨å®é™…çš„LLMåº”ç”¨ä¸­ï¼Œåºåˆ—é•¿åº¦çš„é€‰æ‹©å¾€å¾€å—åˆ°ä»¥ä¸‹å› ç´ çš„é™åˆ¶ï¼š1ï¼‰æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦é™åˆ¶ï¼›2ï¼‰å¯ç”¨GPUå†…å­˜å¤§å°ï¼›3ï¼‰æ¨ç†å»¶è¿Ÿè¦æ±‚ï¼›4ï¼‰æ‰¹å¤„ç†å¤§å°ç­‰ã€‚

        ä¸ºäº†ä¼˜åŒ–é•¿åºåˆ—å¤„ç†ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†å¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–ã€å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ã€åºåˆ—å¹¶è¡ŒåŒ–ç­‰ã€‚è¿™äº›æŠ€æœ¯æœ‰åŠ©äºåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æé«˜è®¡ç®—æ•ˆç‡ã€‚

        åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šè®°å½•è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ¯ä¸ªtokençš„ç”Ÿæˆæ—¶é—´ã€GPUåŠŸè€—å˜åŒ–ã€å†…å­˜ä½¿ç”¨æƒ…å†µç­‰ã€‚è¿™äº›æ•°æ®å¯¹äºç†è§£æ¨¡å‹çš„è¡Œä¸ºå’Œä¼˜åŒ–éƒ¨ç½²ç­–ç•¥éå¸¸é‡è¦ã€‚

        å¡«å……æ–‡æœ¬ç»§ç»­ï¼šåœ¨å®é™…çš„LLMéƒ¨ç½²ä¸­ï¼Œåºåˆ—é•¿åº¦çš„é€‰æ‹©æ˜¯ä¸€ä¸ªéœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªå› ç´ çš„å†³ç­–è¿‡ç¨‹ã€‚ä¸åŒçš„åº”ç”¨åœºæ™¯å¯èƒ½æœ‰ä¸åŒçš„åºåˆ—é•¿åº¦éœ€æ±‚ã€‚

        å¯¹äºå¯¹è¯ç³»ç»Ÿï¼Œé€šå¸¸éœ€è¦è¾ƒé•¿çš„åºåˆ—æ¥ç»´æŒå¯¹è¯çš„ä¸Šä¸‹æ–‡ã€‚å¯¹äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦å¤„ç†è¾ƒé•¿çš„ä»£ç æ–‡ä»¶ã€‚å¯¹äºæ–‡æ¡£æ‘˜è¦ä»»åŠ¡ï¼Œè¾“å…¥åºåˆ—å¯èƒ½åŒ…å«æ•´ä¸ªæ–‡æ¡£çš„å†…å®¹ã€‚

        å› æ­¤ï¼Œåœ¨è®¾è®¡å’Œæµ‹è¯•LLMç³»ç»Ÿæ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘å„ç§å¯èƒ½çš„åºåˆ—é•¿åº¦ï¼Œå¹¶ç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿåœ¨è¿™äº›æƒ…å†µä¸‹ç¨³å®šè¿è¡Œã€‚

        æµ‹è¯•æ•°æ®å¡«å……ç»“æŸï¼šè¿™æ˜¯å¡«å……æ–‡æœ¬çš„æœ€åéƒ¨åˆ†ï¼Œç”¨äºç¡®ä¿æç¤ºè¯è¾¾åˆ°æŒ‡å®šçš„é•¿åº¦è¦æ±‚ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„æ€§èƒ½è¡¨ç°ã€‚
        """
        
        # è®¡ç®—éœ€è¦æ·»åŠ çš„å¡«å……æ–‡æœ¬é•¿åº¦
        remaining_chars = target_chars - current_length
        if remaining_chars > 0:
            # é‡å¤å¡«å……æ–‡æœ¬ç›´åˆ°è¾¾åˆ°ç›®æ ‡é•¿åº¦
            padded_prompt = prompt
            while len(padded_prompt) < target_chars:
                padded_prompt += padding_text
            return padded_prompt[:target_chars]
        
        return prompt
    
    # å°†æ¯ä¸ªæç¤ºè¯å¡«å……åˆ°æŒ‡å®šé•¿åº¦
    test_length = int(args.test_length)
    padded_prompts = [pad_prompt_to_length(prompt, test_length) for prompt in base_prompts]
    
    # æ ¹æ®è¯·æ±‚æ•°é‡ç”Ÿæˆæç¤ºè¯
    test_prompts = (padded_prompts * ((args.requests // len(padded_prompts)) + 1))[:args.requests]
    
    # åˆ›å»ºå­˜å‚¨ç›®å½•
    storage_dir = Path(args.output_dir)
    storage_dir.mkdir(exist_ok=True)
    
    # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
    print(f"\nå¼€å§‹æ‰¹é‡æµ‹è¯•...")
    batch_stats = client.generate_batch(test_prompts, max_tokens=args.max_tokens, batch_size=args.batch_size)
    
    # ä¿å­˜ç»“æœ
    with open(storage_dir / "batch_results.json", "w", encoding="utf-8") as f:
        json.dump(batch_stats, f, ensure_ascii=False, indent=2)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ° {storage_dir} ç›®å½•")

if __name__ == "__main__":
    main()
