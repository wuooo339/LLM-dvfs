#!/usr/bin/env python3
"""
ç®€åŒ–çš„VLLMæ‰¹é‡æµ‹è¯•è„šæœ¬
åªä¿ç•™æ‰¹æ¬¡è°ƒèŠ‚åŠŸèƒ½ï¼Œæ‰¹æ¬¡æ•°å°±æ˜¯åŒæ—¶å‘é€çš„è¯·æ±‚æ•°
"""

import requests
import json
import time
import threading
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ gpu_monitor
sys.path.append(str(Path(__file__).parent.parent.parent))
from gpu_monitor import GPUMonitor

class SimpleBatchClient:
    """ç®€åŒ–çš„æ‰¹é‡æµ‹è¯•å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def health_check(self):
        """æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.base_url}/health")
            return response.status_code == 200
        except Exception as e:
            print(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def generate_single(self, prompt, max_tokens=64, temperature=0.7, request_id=None):
        """å‘é€å•ä¸ªæµå¼ç”Ÿæˆè¯·æ±‚"""
        payload = {
            "model": "/home/wzk/deepseek-v2-lite",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True
        }
        
        # ç«‹å³æ˜¾ç¤ºè¯·æ±‚å¼€å§‹
        print(f"\n[è¯·æ±‚ {request_id+1}] å¼€å§‹å¤„ç†...")
        print(f"  æç¤ºè¯: {prompt[:60]}{'...' if len(prompt) > 60 else ''}")
        print(f"  ç”Ÿæˆå†…å®¹: ", end="")
        sys.stdout.flush()
        
        try:
            start_time = time.time()
            first_token_time = None
            token_count = 0
            generated_text = ""
            token_times = []
            
            # æ ¹æ®max_tokensåŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
            timeout = max(30, max_tokens * 2)
            response = self.session.post(
                f"{self.base_url}/v1/completions",
                json=payload,
                timeout=timeout,
                stream=True
            )
            response.raise_for_status()
            
            # å¤„ç†æµå¼å“åº”
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data_str = line[6:]
                        if data_str.strip() == '[DONE]':
                            break
                        try:
                            data = json.loads(data_str)
                            choices = data.get('choices', [])
                            if choices:
                                choice = choices[0]
                                text_content = choice.get('text', '')
                                if text_content:
                                    current_time = time.time()
                                    token_count += 1
                                    
                                    # è®°å½•é¦–tokenæ—¶é—´
                                    if first_token_time is None:
                                        first_token_time = current_time - start_time
                                    
                                    # è®°å½•æ¯ä¸ªtokençš„æ—¶é—´
                                    token_times.append(current_time - start_time)
                                    
                                    generated_text += text_content
                                    
                                    # å®æ—¶æ˜¾ç¤ºtokenå†…å®¹å’Œæ—¶é—´ï¼Œæ ¼å¼ç±»ä¼¼test_vllm_unified.py
                                    display_text = text_content.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                                    if len(display_text) > 20:
                                        display_text = display_text[:20] + "..."
                                    print(f"    [è¯·æ±‚ {request_id+1}] Token {token_count}: '{display_text}' (æ—¶é—´: {current_time - start_time:.4f}s)")
                        except json.JSONDecodeError:
                            continue
            
            end_time = time.time()
            
            # è®¡ç®—æ—¶é—´æŒ‡æ ‡
            total_time = end_time - start_time
            ttft = first_token_time if first_token_time else 0
            
            print(f"\n[è¯·æ±‚ {request_id+1}] âœ… å®Œæˆ")
            print(f"  æ€»æ—¶é—´: {total_time:.3f}s")
            print(f"  TTFT: {ttft:.3f}s")
            print(f"  ç”Ÿæˆtokens: {token_count}")
            print("-" * 60)
            sys.stdout.flush()
            
            return {
                'request_id': request_id,
                'prompt': prompt,
                'generated_text': generated_text,
                'response_time': total_time,
                'ttft': ttft,
                'token_count': token_count,
                'token_times': token_times,
                'success': True,
                'error': None
            }
        except Exception as e:
            error_msg = str(e)
            print(f"\n[è¯·æ±‚ {request_id+1}] âŒ å¤±è´¥")
            print(f"  é”™è¯¯: {error_msg}")
            print("-" * 60)
            sys.stdout.flush()
            return {
                'request_id': request_id,
                'prompt': prompt,
                'generated_text': '',
                'response_time': 0,
                'ttft': 0,
                'token_count': 0,
                'token_times': [],
                'success': False,
                'error': error_msg
            }
    
    def generate_batch(self, prompts, max_tokens=64, temperature=0.7, batch_size=None):
        """æ‰¹é‡ç”Ÿæˆè¯·æ±‚ - æ‰¹æ¬¡æ•°å°±æ˜¯åŒæ—¶å‘é€çš„è¯·æ±‚æ•°"""
        if batch_size is None:
            batch_size = len(prompts)
        
        print(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆæµ‹è¯•")
        print(f"æ€»è¯·æ±‚æ•°: {len(prompts)}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size} (åŒæ—¶å‘é€çš„è¯·æ±‚æ•°)")
        print(f"æœ€å¤§tokens: {max_tokens}")
        print("=" * 60)
        
        # æ‰“å°æ‰€æœ‰æç¤ºè¯
        for i, prompt in enumerate(prompts):
            print(f"è¯·æ±‚ {i+1}: {prompt}")
        print("=" * 60)
        
        all_results = []
        start_time = time.time()
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for batch_start in range(0, len(prompts), batch_size):
            batch_end = min(batch_start + batch_size, len(prompts))
            batch_prompts = prompts[batch_start:batch_end]
            
            print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}: è¯·æ±‚ {batch_start+1}-{batch_end}")
            print("=" * 60)
            
            # ä½¿ç”¨ThreadPoolExecutorå¤„ç†å½“å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=batch_size) as executor:
                # æäº¤å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä»»åŠ¡
                future_to_prompt = {
                    executor.submit(self.generate_single, prompt, max_tokens, temperature, batch_start + i): prompt
                    for i, prompt in enumerate(batch_prompts)
                }
                
                # å®æ—¶ç­‰å¾…å¹¶æ˜¾ç¤ºç»“æœ
                batch_results = []
                completed_count = 0
                for future in as_completed(future_to_prompt):
                    result = future.result()
                    batch_results.append(result)
                    all_results.append(result)
                    completed_count += 1
                    
                    # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                    print(f"ğŸ“Š æ‰¹æ¬¡è¿›åº¦: {completed_count}/{len(batch_prompts)} å®Œæˆ")
                    sys.stdout.flush()
                
                # æ‰“å°æ‰¹æ¬¡ç»Ÿè®¡
                successful = sum(1 for r in batch_results if r['success'])
                failed = len(batch_results) - successful
                avg_time = sum(r['response_time'] for r in batch_results if r['success']) / max(successful, 1)
                
                print(f"\næ‰¹æ¬¡ {batch_start//batch_size + 1} å®Œæˆ:")
                print(f"  æˆåŠŸ: {successful}/{len(batch_prompts)}")
                print(f"  å¤±è´¥: {failed}")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
                print("=" * 60)
        
        total_time = time.time() - start_time
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        successful_results = [r for r in all_results if r['success']]
        failed_results = [r for r in all_results if not r['success']]
        
        if successful_results:
            response_times = [r['response_time'] for r in successful_results]
            ttft_times = [r['ttft'] for r in successful_results if 'ttft' in r]
            token_counts = [r['token_count'] for r in successful_results if 'token_count' in r]
            
            avg_response_time = sum(response_times) / len(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            throughput = len(successful_results) / total_time
            
            if ttft_times:
                avg_ttft = sum(ttft_times) / len(ttft_times)
                min_ttft = min(ttft_times)
                max_ttft = max(ttft_times)
            else:
                avg_ttft = min_ttft = max_ttft = 0
                
            if token_counts:
                avg_tokens = sum(token_counts) / len(token_counts)
                total_tokens = sum(token_counts)
            else:
                avg_tokens = total_tokens = 0
        else:
            avg_response_time = min_response_time = max_response_time = 0
            avg_ttft = min_ttft = max_ttft = 0
            avg_tokens = total_tokens = 0
            throughput = 0
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print(f"\næœ€ç»ˆç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {len(prompts)}")
        print(f"  æˆåŠŸè¯·æ±‚: {len(successful_results)}")
        print(f"  å¤±è´¥è¯·æ±‚: {len(failed_results)}")
        print(f"  æˆåŠŸç‡: {len(successful_results)/len(prompts)*100:.1f}%")
        print(f"  æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        print(f"  æœ€å°å“åº”æ—¶é—´: {min_response_time:.3f}s")
        print(f"  æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.3f}s")
        print(f"  å¹³å‡TTFT: {avg_ttft:.3f}s")
        print(f"  æœ€å°TTFT: {min_ttft:.3f}s")
        print(f"  æœ€å¤§TTFT: {max_ttft:.3f}s")
        print(f"  æ€»ç”Ÿæˆtokens: {total_tokens}")
        print(f"  å¹³å‡tokens/è¯·æ±‚: {avg_tokens:.1f}")
        print(f"  ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        
        return {
            'total_requests': len(prompts),
            'successful_requests': len(successful_results),
            'failed_requests': len(failed_results),
            'total_time': total_time,
            'avg_response_time': avg_response_time,
            'min_response_time': min_response_time,
            'max_response_time': max_response_time,
            'throughput': throughput,
            'results': all_results
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç®€åŒ–çš„VLLMæ‰¹é‡æµ‹è¯•å·¥å…·')
    parser.add_argument('--batch-size', type=int, required=True, help='æ‰¹æ¬¡å¤§å°ï¼ˆåŒæ—¶å‘é€çš„è¯·æ±‚æ•°ï¼‰')
    parser.add_argument('--requests', type=int, default=8, help='æ€»è¯·æ±‚æ•°')
    parser.add_argument('--max-tokens', type=int, default=64, help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    parser.add_argument('--output-dir', type=str, default='simple_batch_results', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("ç®€åŒ–çš„VLLMæ‰¹é‡æµ‹è¯•å·¥å…·")
    print("=" * 50)
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"æ€»è¯·æ±‚æ•°: {args.requests}")
    print(f"æœ€å¤§tokens: {args.max_tokens}")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = SimpleBatchClient("http://localhost:8000")
    
    # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
    print("\næ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€...")
    if not client.health_check():
        print("âŒ æœåŠ¡å™¨æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨ VLLM æœåŠ¡å™¨")
        print("è¿è¡Œå‘½ä»¤: ./start_vllm_server.sh")
        return
    
    print("âœ… æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
    
    # ç”Ÿæˆæµ‹è¯•æç¤ºè¯
    base_prompts = [
        "1. è‹¥3å°æœºå™¨5å°æ—¶ç”Ÿäº§180ä¸ªé›¶ä»¶ï¼Œ7å°æœºå™¨8å°æ—¶å¯ç”Ÿäº§å¤šå°‘é›¶ä»¶ï¼Ÿ",
        "2. ç”²æ¯”ä¹™å¤§6å²ï¼Œ5å¹´å‰ç”²å¹´é¾„æ˜¯ä¹™çš„2å€ï¼Œæ±‚ä¸¤äººç°åœ¨å¹´é¾„ã€‚",
        "3. ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹",
        "4. è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆç°è±¡",
        "5. ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ",
        "6. å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
        "7. è§£é‡Šä»€ä¹ˆæ˜¯å¾®æœåŠ¡æ¶æ„",
        "8. ä»€ä¹ˆæ˜¯å®¹å™¨åŒ–æŠ€æœ¯ï¼Ÿ"
    ]
    
    # æ ¹æ®è¯·æ±‚æ•°é‡ç”Ÿæˆæç¤ºè¯
    test_prompts = (base_prompts * ((args.requests // len(base_prompts)) + 1))[:args.requests]
    
    # åˆ›å»ºå­˜å‚¨ç›®å½•
    storage_dir = Path(args.output_dir)
    storage_dir.mkdir(exist_ok=True)
    
    # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
    print(f"\nå¼€å§‹æ‰¹é‡æµ‹è¯•...")
    batch_stats = client.generate_batch(test_prompts, max_tokens=args.max_tokens, batch_size=args.batch_size)
    
    # ä¿å­˜ç»“æœ
    with open(storage_dir / "batch_results.json", "w", encoding="utf-8") as f:
        json.dump(batch_stats, f, ensure_ascii=False, indent=2)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ° {storage_dir} ç›®å½•")

if __name__ == "__main__":
    main()
