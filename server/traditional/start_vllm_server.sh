#!/bin/bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3  # è®¾ç½®ä½¿ç”¨çš„ GPU
export VLLM_USE_MODELSCOPE=False
export VLLM_WORKER_MULTIPROC_METHOD=spawn
export VLLM_ENGINE_MULTIPROC_METHOD=spawn
export CUDA_LAUNCH_BLOCKING=1
export LD_LIBRARY_PATH="/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
export TRITON_CACHE_DIR="/tmp/triton_cache"

echo "ğŸ”§ å¯åŠ¨ VLLM æœåŠ¡å™¨..."


#å¯åŠ¨Tensorå¹¶è¡Œ
vllm serve /share-data/wzk-1/model/deepseek-v2-lite \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 32768 \
    --max-num-batched-tokens 131072 \
    --max-num-seqs 256 \
    --block-size 128
    
#å¯åŠ¨å¸è½½æ‰§è¡Œ
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
vllm serve /share-data/wzk-1/model/deepseek-v2-lite \
    --host 0.0.0.0 \
    --port 8000 \
    --cpu-offload-gb 20 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 4096 \
    --max-num-batched-tokens 65536 \
    --max-num-seqs 16 \
    --block-size 128 \
    --enforce-eager