#!/usr/bin/env python3
"""
BIG-Bench Hard (BBH) æ•°æ®é›†æµ‹è¯•è„šæœ¬
ä½¿ç”¨BBHæ•°æ®é›†è¿›è¡Œé•¿è¾“å…¥æµ‹è¯•ï¼Œæµ‹é‡åŠŸè€—å’Œæ€§èƒ½
"""

import requests
import time
import json
import random
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))
from gpu_monitor import GPUMonitor

# å°è¯•å¯¼å…¥datasetsåº“
try:
    from datasets import load_dataset
    DATASETS_AVAILABLE = True
except ImportError:
    print("âš ï¸  datasetsåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®")
    print("   å®‰è£…å‘½ä»¤: pip install datasets")
    DATASETS_AVAILABLE = False

class BBHTestClient:
    """BBHæ•°æ®é›†æµ‹è¯•å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            "Content-Type": "application/json",
            "User-Agent": "BBH-Test-Client/1.0"
        })
    
    def health_check(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=10)
            return response.status_code == 200
        except Exception as e:
            print(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def generate_response(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> Optional[Dict]:
        """å‘é€ç”Ÿæˆè¯·æ±‚"""
        payload = {
            "model": "/share-data/wzk-1/model/DeepSeek-R1-Distill-Qwen-32B",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": False
        }
        
        try:
            start_time = time.time()
            response = self.session.post(
                f"{self.base_url}/v1/completions",
                json=payload,
                timeout=120
            )
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json()
                
                # å®æ—¶æ‰“å°promptå’Œè¾“å‡ºä¿¡æ¯
                prompt_tokens = result.get('usage', {}).get('prompt_tokens', 0)
                completion_tokens = result.get('usage', {}).get('completion_tokens', 0)
                total_tokens = result.get('usage', {}).get('total_tokens', 0)
                response_text = result.get('choices', [{}])[0].get('text', '')
                
                print(f"      ğŸ“ Prompt tokens: {prompt_tokens}")
                print(f"      ğŸ“¤ Completion tokens: {completion_tokens}")
                print(f"      ğŸ“Š Total tokens: {total_tokens}")
                print(f"      â±ï¸  Request time: {end_time - start_time:.2f}s")
                print(f"      ğŸ’¬ Response preview: {response_text[:100]}{'...' if len(response_text) > 100 else ''}")
                
                return {
                    "response": result,
                    "request_time": end_time - start_time,
                    "timestamp": start_time,
                    "prompt_length": len(prompt),
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens
                }
            else:
                print(f"è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return None
        except Exception as e:
            print(f"ç”Ÿæˆè¯·æ±‚å¤±è´¥: {e}")
            return None

    def generate_stream(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7):
        """å‘é€æµå¼ç”Ÿæˆè¯·æ±‚"""
        payload = {
            "model": "/share-data/wzk-1/model/DeepSeek-R1-Distill-Qwen-32B",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True
        }

        # ç®€å•é‡è¯•ï¼šåœ¨å¹¶å‘é«˜æ—¶æé«˜ç¨³å¥æ€§
        last_err = None
        for attempt in range(3):
            try:
                response = self.session.post(
                    f"{self.base_url}/v1/completions",
                    json=payload,
                    timeout=300,
                    stream=True
                )
                response.raise_for_status()
                return response
            except Exception as e:
                last_err = e
                print(f"æµå¼ç”Ÿæˆè¯·æ±‚å¤±è´¥(ç¬¬{attempt+1}æ¬¡): {e}")
                time.sleep(1.0 * (attempt + 1))
        print(f"æµå¼ç”Ÿæˆè¯·æ±‚æœ€ç»ˆå¤±è´¥: {last_err}")
        return None

def load_bbh_tasks() -> List[Dict[str, Any]]:
    """åŠ è½½BBHä»»åŠ¡æ•°æ®"""
    if DATASETS_AVAILABLE:
        return load_bbh_tasks_from_official()
    else:
        return load_bbh_tasks_fallback()

def load_bbh_tasks_from_official() -> List[Dict[str, Any]]:
    """ä»å®˜æ–¹æ•°æ®é›†åŠ è½½BBHä»»åŠ¡"""
    print("ğŸ“š ä»å®˜æ–¹BIG-Bench Hardæ•°æ®é›†åŠ è½½ä»»åŠ¡...")
    
    # BBHå®˜æ–¹æ•°æ®é›†ä¸­çš„ä¸»è¦ä»»åŠ¡
    bbh_task_names = [
        "logical_deduction_three_objects",
        "date_understanding", 
        "causal_judgment",
        "disambiguation_qa",
        "geometric_shapes",
        "logical_deduction_five_objects",
        "logical_deduction_seven_objects",
        "multistep_arithmetic_two",
        "reasoning_about_colored_objects",
        "navigate",
        "ruin_names",
        "snarks",
        "sports_understanding",
        "temporal_sequences",
        "tracking_shuffled_objects_five_objects",
        "tracking_shuffled_objects_seven_objects",
        "tracking_shuffled_objects_three_objects"
    ]
    
    bbh_tasks = []
    
    for task_name in bbh_task_names:
        try:
            print(f"  åŠ è½½ä»»åŠ¡: {task_name}")
            # åŠ è½½ç‰¹å®šä»»åŠ¡
            dataset = load_dataset("bigbenchhard", task_name)
            
            # è·å–è®­ç»ƒæ•°æ®
            train_data = dataset['train']
            
            # éšæœºé€‰æ‹©ä¸€äº›ç¤ºä¾‹ï¼ˆæœ€å¤š5ä¸ªï¼‰
            examples = []
            if len(train_data) > 0:
                # éšæœºé€‰æ‹©ç¤ºä¾‹
                selected_indices = random.sample(range(len(train_data)), min(5, len(train_data)))
                for idx in selected_indices:
                    example = train_data[idx]
                    examples.append({
                        "input": example['input'],
                        "target": example['target']
                    })
            
            if examples:
                bbh_tasks.append({
                    "task_name": task_name,
                    "description": f"BIG-Bench Hard: {task_name}",
                    "examples": examples,
                    "total_examples": len(train_data)
                })
                print(f"    âœ… åŠ è½½äº† {len(examples)} ä¸ªç¤ºä¾‹ (æ€»å…± {len(train_data)} ä¸ª)")
            else:
                print(f"    âš ï¸  ä»»åŠ¡ {task_name} æ²¡æœ‰å¯ç”¨ç¤ºä¾‹")
                
        except Exception as e:
            print(f"    âŒ åŠ è½½ä»»åŠ¡ {task_name} å¤±è´¥: {e}")
            continue
    
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(bbh_tasks)} ä¸ªBBHä»»åŠ¡")
    return bbh_tasks

def load_bbh_tasks_fallback() -> List[Dict[str, Any]]:
    """åŠ è½½å†…ç½®çš„BBHä»»åŠ¡ç¤ºä¾‹æ•°æ®ï¼ˆå½“å®˜æ–¹æ•°æ®é›†ä¸å¯ç”¨æ—¶ï¼‰"""
    print("ğŸ“š ä½¿ç”¨å†…ç½®BBHä»»åŠ¡ç¤ºä¾‹æ•°æ®...")
    bbh_tasks = [
        {
            "task_name": "causal_judgment",
            "description": "å› æœåˆ¤æ–­ä»»åŠ¡",
            "examples": [
                {
                    "input": "A man named John goes to a restaurant and orders a hamburger. The hamburger is poisoned. John eats the hamburger and dies. What was the cause of John's death?",
                    "target": "The poisoned hamburger"
                },
                {
                    "input": "A woman named Sarah is driving her car on a rainy day. She hits a patch of ice and crashes into a tree. What was the cause of the crash?",
                    "target": "The ice on the road"
                }
            ]
        },
        {
            "task_name": "date_understanding",
            "description": "æ—¥æœŸç†è§£ä»»åŠ¡",
            "examples": [
                {
                    "input": "Today is the 1st of January, 2023. What will be the date 3 days from now?",
                    "target": "January 4th, 2023"
                },
                {
                    "input": "If today is March 15th, 2023, what was the date 2 weeks ago?",
                    "target": "March 1st, 2023"
                }
            ]
        },
        {
            "task_name": "disambiguation_qa",
            "description": "æ¶ˆæ­§é—®ç­”ä»»åŠ¡",
            "examples": [
                {
                    "input": "The word 'bank' can refer to a financial institution or the side of a river. In the sentence 'I went to the bank to deposit money', what does 'bank' mean?",
                    "target": "A financial institution"
                },
                {
                    "input": "The word 'bark' can refer to the sound a dog makes or the outer covering of a tree. In the sentence 'The dog's bark woke me up', what does 'bark' mean?",
                    "target": "The sound a dog makes"
                }
            ]
        },
        {
            "task_name": "geometric_shapes",
            "description": "å‡ ä½•å½¢çŠ¶ä»»åŠ¡",
            "examples": [
                {
                    "input": "A square has 4 sides of equal length. If one side is 5 cm long, what is the perimeter of the square?",
                    "target": "20 cm"
                },
                {
                    "input": "A circle has a radius of 3 cm. What is the area of the circle? (Use Ï€ â‰ˆ 3.14)",
                    "target": "28.26 cmÂ²"
                }
            ]
        },
        {
            "task_name": "logical_deduction",
            "description": "é€»è¾‘æ¨ç†ä»»åŠ¡",
            "examples": [
                {
                    "input": "All birds can fly. Penguins are birds. Can penguins fly?",
                    "target": "No, penguins cannot fly (this is a logical paradox)"
                },
                {
                    "input": "If it's raining, then the ground is wet. The ground is wet. Is it raining?",
                    "target": "Not necessarily - the ground could be wet for other reasons"
                }
            ]
        },
        {
            "task_name": "multistep_arithmetic",
            "description": "å¤šæ­¥ç®—æœ¯ä»»åŠ¡",
            "examples": [
                {
                    "input": "Sarah has 12 apples. She gives 3 apples to her friend and buys 7 more apples. How many apples does Sarah have now?",
                    "target": "16 apples"
                },
                {
                    "input": "A store has 50 items. They sell 15 items in the morning and 20 items in the afternoon. How many items are left?",
                    "target": "15 items"
                }
            ]
        },
        {
            "task_name": "navigate",
            "description": "å¯¼èˆªä»»åŠ¡",
            "examples": [
                {
                    "input": "You are at point A. To get to point B, you need to go 3 blocks north, then 2 blocks east, then 1 block south. What direction should you start walking?",
                    "target": "North"
                },
                {
                    "input": "Starting from home, you walk 2 blocks west, then 3 blocks north, then 1 block east. How many blocks are you from home?",
                    "target": "3 blocks (using Pythagorean theorem: âˆš(1Â² + 3Â²) = âˆš10 â‰ˆ 3.16)"
                }
            ]
        },
        {
            "task_name": "reasoning_about_colored_objects",
            "description": "é¢œè‰²å¯¹è±¡æ¨ç†ä»»åŠ¡",
            "examples": [
                {
                    "input": "There are 3 boxes: a red box, a blue box, and a green box. The red box contains a ball. The blue box is empty. What color is the box that contains the ball?",
                    "target": "Red"
                },
                {
                    "input": "A red car and a blue car are parked. The red car is faster than the blue car. Which car is faster?",
                    "target": "The red car"
                }
            ]
        },
        {
            "task_name": "ruin_names",
            "description": "åå­—æ¨ç†ä»»åŠ¡",
            "examples": [
                {
                    "input": "If you rearrange the letters in 'LISTEN', what word do you get?",
                    "target": "SILENT"
                },
                {
                    "input": "What word can be formed by rearranging the letters in 'HEART'?",
                    "target": "EARTH"
                }
            ]
        },
        {
            "task_name": "snarks",
            "description": "è®½åˆºç†è§£ä»»åŠ¡",
            "examples": [
                {
                    "input": "John said 'Great weather we're having' while it was pouring rain. What did John really mean?",
                    "target": "He was being sarcastic - the weather is terrible"
                },
                {
                    "input": "Sarah said 'Oh, that's just perfect' when her computer crashed. What did Sarah really mean?",
                    "target": "She was being sarcastic - this is the worst possible timing"
                }
            ]
        }
    ]
    
    return bbh_tasks

def create_long_prompt(task: Dict[str, Any], example: Dict[str, str], context_length: int = 2000) -> str:
    """åˆ›å»ºé•¿è¾“å…¥æç¤º"""
    base_prompt = f"""
Task: {task['description']}
Task Name: {task['task_name']}

Instructions: Please solve the following problem step by step. Think carefully and provide a clear, logical answer.

Problem: {example['input']}

Please provide your answer and explain your reasoning process.
"""
    
    # æ·»åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡æ¥å¢åŠ è¾“å…¥é•¿åº¦
    context_padding = f"""

Additional Context:
This is a complex reasoning task that requires careful analysis. You should:
1. Read the problem carefully
2. Identify the key information
3. Apply logical reasoning
4. Consider alternative interpretations
5. Provide a well-reasoned answer

The problem involves multiple steps of reasoning and may require you to consider various factors before arriving at the correct answer. Take your time to think through each step carefully.

Remember to:
- Break down complex problems into smaller parts
- Consider all given information
- Apply logical rules consistently
- Check your reasoning for errors
- Provide clear explanations for your answer

Now, please solve the problem:
"""
    
    # å¦‚æœä¸Šä¸‹æ–‡è¿˜ä¸å¤Ÿé•¿ï¼Œæ·»åŠ æ›´å¤šå†…å®¹
    while len(base_prompt + context_padding) < context_length:
        context_padding += f"""

Additional reasoning guidelines:
- Pay attention to details in the problem statement
- Consider edge cases and exceptions
- Use systematic approaches to problem-solving
- Verify your answer makes logical sense
- Consider alternative interpretations of the problem

The key to solving this type of problem is to be methodical and thorough in your analysis.
"""
    
    return base_prompt + context_padding

def run_bbh_test(client: BBHTestClient, gpu_monitor: GPUMonitor, limit: int = 10, context_length: int = 2000, max_tokens: int = 512) -> Dict[str, Any]:
    """è¿è¡ŒBBHæµ‹è¯•ï¼ˆä»…é€‰å–æŒ‡å®šæ•°é‡çš„æ•°æ®ï¼Œæµå¼æ‰“å°tokenä¸æ—¶é—´ï¼‰"""
    print("ğŸ§  å¼€å§‹BBHæ•°æ®é›†æµ‹è¯•...")

    # åŠ è½½BBHä»»åŠ¡
    bbh_tasks = load_bbh_tasks()
    print(f"ğŸ“š åŠ è½½äº† {len(bbh_tasks)} ä¸ªBBHä»»åŠ¡")

    # é¢„å…ˆé€‰å–ä¸è¶…è¿‡ limit ä¸ªç¤ºä¾‹ï¼Œè·¨ä»»åŠ¡éšæœºé‡‡æ ·
    selected_items: List[Dict[str, Any]] = []
    task_pool = bbh_tasks.copy()
    random.shuffle(task_pool)
    for task in task_pool:
        examples = task.get('examples', [])
        random.shuffle(examples)
        for example in examples:
            long_prompt = create_long_prompt(task, example, context_length)
            selected_items.append({
                "task_name": task['task_name'],
                "description": task['description'],
                "prompt": long_prompt
            })
            if len(selected_items) >= limit:
                break
        if len(selected_items) >= limit:
            break

    print(f"ğŸ”¢ å°†æµ‹è¯• {len(selected_items)} æ¡æ•°æ®ï¼ˆä¸Šé™ {limit}ï¼‰")

    results: Dict[str, Any] = {
        "test_info": {
            "dataset": "BIG-Bench Hard (BBH)",
            "context_length": context_length,
            "limit": limit,
            "timestamp": time.time()
        },
        "requests": [],
        "gpu_data": []
    }

    # å¼€å§‹GPUç›‘æ§
    gpu_monitor.start_monitoring()

    for idx, item in enumerate(selected_items, start=1):
        prompt = item["prompt"]
        print(f"\n=== è¯·æ±‚ {idx}/{len(selected_items)} - ä»»åŠ¡: {item['task_name']} ===")
        print(f"ğŸ“ Prompt (length={len(prompt)}):\n{prompt}")

        request_start = time.time()
        first_token_time = None
        token_times: List[float] = []
        token_count = 0
        generated_text = ""

        # æµå¼è¯·æ±‚
        response = client.generate_stream(prompt=prompt, max_tokens=max_tokens, temperature=0.7)
        if not response:
            print("âŒ è¯·æ±‚å¤±è´¥")
            results["requests"].append({
                "task_name": item['task_name'],
                "description": item['description'],
                "prompt_length": len(prompt),
                "error": "request_failed"
            })
            continue

        try:
            for line in response.iter_lines():
                if not line:
                    continue
                line = line.decode('utf-8')
                if not line.startswith('data: '):
                    continue
                data_str = line[6:]
                if data_str.strip() == '[DONE]':
                    break
                try:
                    data = json.loads(data_str)
                except json.JSONDecodeError:
                    continue
                choices = data.get('choices', [])
                if not choices:
                    continue
                text_content = choices[0].get('text', '')
                if not text_content:
                    continue
                now = time.time()
                token_count += 1
                if first_token_time is None:
                    first_token_time = now - request_start
                token_times.append(now - request_start)
                generated_text += text_content
                display_text = text_content.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                if len(display_text) > 40:
                    display_text = display_text[:40] + "..."
                print(f"    Token {token_count}: '{display_text}' @ {now - request_start:.4f}s")
        except Exception as e:
            print(f"  å¤„ç†æµå¼å“åº”æ—¶å‡ºé”™: {e}")

        total_time = time.time() - request_start
        ttft = first_token_time or 0.0
        if token_count > 1 and len(token_times) > 1:
            tpot = (token_times[-1] - token_times[0]) / (len(token_times) - 1)
        else:
            tpot = 0.0

        results["requests"].append({
            "task_name": item['task_name'],
            "description": item['description'],
            "prompt_length": len(prompt),
            "response_preview": generated_text[:200],
            "token_count": token_count,
            "ttft": ttft,
            "tpot": tpot,
            "total_time": total_time,
            "token_times": token_times
        })

        print(f"  å®Œæˆ: æ€»æ—¶é—´={total_time:.4f}s, TTFT={ttft:.4f}s, TPOT={tpot:.4f}s, Tokens={token_count}")

        # è½»å¾®é—´éš”ï¼Œé¿å…è¿‡äºå¯†é›†
        time.sleep(0.2)

    # åœæ­¢GPUç›‘æ§
    gpu_monitor.stop_monitoring()
    results['gpu_data'] = gpu_monitor.get_data()

    return results

def analyze_bbh_results(results: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ†æBBHæµ‹è¯•ç»“æœ"""
    analysis = {
        "summary": {},
        "context_length_analysis": {},
        "task_performance": {},
        "gpu_analysis": {}
    }
    
    # åŸºæœ¬ç»Ÿè®¡
    total_tasks = len(results['task_results'])
    total_requests = sum(len(task['context_length_results']) for task in results['task_results'])
    successful_requests = sum(
        len([r for r in task['context_length_results'] if 'error' not in r])
        for task in results['task_results']
    )
    
    analysis['summary'] = {
        "total_tasks": total_tasks,
        "total_requests": total_requests,
        "successful_requests": successful_requests,
        "success_rate": successful_requests / total_requests if total_requests > 0 else 0
    }
    
    # æŒ‰ä¸Šä¸‹æ–‡é•¿åº¦åˆ†æ
    context_lengths = results['test_info']['context_lengths']
    for context_length in context_lengths:
        context_results = []
        for task in results['task_results']:
            for result in task['context_length_results']:
                if result['context_length'] == context_length and 'error' not in result:
                    context_results.append(result)
        
        if context_results:
            analysis['context_length_analysis'][str(context_length)] = {
                "count": len(context_results),
                "avg_request_time": sum(r['request_time'] for r in context_results) / len(context_results),
                "avg_prompt_tokens": sum(r['prompt_tokens'] for r in context_results) / len(context_results),
                "avg_completion_tokens": sum(r['completion_tokens'] for r in context_results) / len(context_results),
                "avg_total_tokens": sum(r['total_tokens'] for r in context_results) / len(context_results)
            }
    
    # GPUåˆ†æ
    if results['gpu_data']:
        gpu_data = results['gpu_data']
        powers = [entry.get('power_draw', 0) for entry in gpu_data if entry.get('power_draw', 0) > 0]
        utilizations = [entry.get('gpu_utilization', 0) for entry in gpu_data if entry.get('gpu_utilization', 0) > 0]
        
        analysis['gpu_analysis'] = {
            "avg_power": sum(powers) / len(powers) if powers else 0,
            "max_power": max(powers) if powers else 0,
            "min_power": min(powers) if powers else 0,
            "avg_utilization": sum(utilizations) / len(utilizations) if utilizations else 0,
            "max_utilization": max(utilizations) if utilizations else 0,
            "data_points": len(gpu_data)
        }
    
    return analysis

def save_results(results: Dict[str, Any], analysis: Dict[str, Any], storage_dir: Path):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    storage_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜åŸå§‹ç»“æœ
    with open(storage_dir / 'bbh_test_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜åˆ†æç»“æœ
    with open(storage_dir / 'bbh_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ° {storage_dir} ç›®å½•")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  BIG-Bench Hard (BBH) æ•°æ®é›†æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = BBHTestClient()
    
    # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
    if not client.health_check():
        print("âŒ VLLM æœåŠ¡å™¨æœªè¿è¡Œ")
        print("è¯·å…ˆè¿è¡Œ: ./start_vllm_server.sh")
        return
    
    print("âœ… VLLM æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
    
    # åˆ›å»ºGPUç›‘æ§å™¨
    gpu_monitor = GPUMonitor(interval=0.1)
    
    # è¿è¡Œæµ‹è¯•ï¼ˆä»…å–10æ¡ï¼Œæµå¼æ‰“å°tokenä¸æ—¶é—´ï¼‰
    results = run_bbh_test(client, gpu_monitor, limit=10, context_length=2000, max_tokens=512)
    
    # åˆ†æç»“æœ
    print("\nğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")
    analysis = analyze_bbh_results(results)
    
    # ä¿å­˜ç»“æœ
    storage_dir = Path("bbh_test_results")
    save_results(results, analysis, storage_dir)
    
    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ“ˆ æµ‹è¯•æ‘˜è¦:")
    print(f"  æ€»ä»»åŠ¡æ•°: {analysis['summary']['total_tasks']}")
    print(f"  æ€»è¯·æ±‚æ•°: {analysis['summary']['total_requests']}")
    print(f"  æˆåŠŸè¯·æ±‚æ•°: {analysis['summary']['successful_requests']}")
    print(f"  æˆåŠŸç‡: {analysis['summary']['success_rate']:.2%}")
    
    if analysis['gpu_analysis']:
        print(f"\nâš¡ GPU æ€§èƒ½:")
        print(f"  å¹³å‡åŠŸè€—: {analysis['gpu_analysis']['avg_power']:.1f}W")
        print(f"  æœ€å¤§åŠŸè€—: {analysis['gpu_analysis']['max_power']:.1f}W")
        print(f"  å¹³å‡åˆ©ç”¨ç‡: {analysis['gpu_analysis']['avg_utilization']:.1f}%")
        print(f"  æœ€å¤§åˆ©ç”¨ç‡: {analysis['gpu_analysis']['max_utilization']:.1f}%")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {storage_dir}")
    print("ğŸ‰ BBHæµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()
